<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<title>slon</title>
<link rel="stylesheet" href="stylesheet.css" type="text/css">
<link rev="made" href="pgsql-docs@postgresql.org">
<meta name="generator" content="DocBook XSL Stylesheets V1.66.1">
<link rel="start" href="index.html" title="Slony-I HEAD_20050707 Documentation">
<link rel="up" href="commandreference.html" title="Part I. Core Slony-I Programs">
<link rel="prev" href="commandreference.html" title="Part I. Core Slony-I Programs">
<link rel="next" href="runtime-config.html" title="Run-time Configuration">
</head>
<body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="refentry" lang="en">
<a name="slon"></a><div class="titlepage"></div>
<div class="refnamediv">
<h2>Name</h2>
<p><span class="application">slon</span> &#8212;    <span class="productname">Slony-I</span> daemon
  </p>
</div>
<a name="id2550624"></a><div class="refsynopsisdiv">
<h2>Synopsis</h2>
<div class="cmdsynopsis"><p><tt class="command">slon</tt> [<i class="replaceable"><tt>option</tt></i>...] [<i class="replaceable"><tt>clustername</tt></i>] [<i class="replaceable"><tt>conninfo</tt></i>]</p></div>
</div>
<div class="refsect1" lang="en">
<a name="id2550672"></a><h2>Description</h2>
<p>   <span class="application">slon</span> is the daemon application that
   &#8220;<span class="quote">runs</span>&#8221; <span class="productname">Slony-I</span> replication.  A
   <span class="application">slon</span> instance must be run for each node
   in a <span class="productname">Slony-I</span> cluster.
  </p>
</div>
<div class="refsect1" lang="en">
<a name="r1-app-slon-3"></a><h2>Options</h2>
<div class="variablelist"><dl>
<dt><span class="term"><tt class="option">-d</tt><i class="replaceable"><tt>debuglevel</tt></i></span></dt>
<dd>
<p>      The <tt class="envar">log_level</tt> specifies the level of verbosity
      that <span class="application">slon</span> should use when logging its
      activity.
     </p>
<p>      The eight levels of logging are:
      </p>
<div class="itemizedlist"><ul type="disc">
<li><p>Error</p></li>
<li><p>Warn</p></li>
<li><p>Config</p></li>
<li><p>Info</p></li>
<li><p>Debug1</p></li>
<li><p>Debug2</p></li>
<li><p>Debug3</p></li>
<li><p>Debug4</p></li>
</ul></div>
<p>
     </p>
</dd>
<dt><span class="term"><tt class="option">-s</tt><i class="replaceable"><tt>SYNC check interval</tt></i></span></dt>
<dd>
<p>      The <tt class="envar">sync_interval</tt>, measured in milliseconds,
      indicates how often <span class="application">slon</span> should check
      to see if a <tt class="command">SYNC</tt> should be introduced.
      Default is 10000 ms.  The main loop in
      <tt class="function">sync_Thread_main()</tt> sleeps for intervals of
      <tt class="envar">sync_interval</tt> milliseconds between iterations.
     </p>
<p>      Short sync check intervals keep the origin on a &#8220;<span class="quote">short
      leash</span>&#8221;, updating its subscribers more frequently.  If you
      have replicated sequences that are frequently updated
      <span class="emphasis"><em>without</em></span> there being tables that are
      affected, this keeps there from being times when only sequences
      are updated, and therefore <span class="emphasis"><em>no</em></span> syncs take
      place
     </p>
<p>      If the node is not an origin for any replication set, so no
      updates are coming in, it is somewhat wasteful for this value to
      be much less the <tt class="envar">sync_interval_timeout</tt> value.
     </p>
</dd>
<dt><span class="term"><tt class="option">-t</tt><i class="replaceable"><tt>SYNC
    interval timeout</tt></i></span></dt>
<dd>
<p>      At the end of each <tt class="envar">sync_interval_timeout</tt> timeout
      period, a <tt class="command">SYNC</tt> will be generated on the
      &#8220;<span class="quote">local</span>&#8221; node even if there has been no replicatable
      data updated that would have pushed out a
      <tt class="command">SYNC</tt>.
     </p>
<p>      Default, and maximum, is 60000 ms, so that you can expect each
      node to &#8220;<span class="quote">report in</span>&#8221; with a <tt class="command">SYNC</tt>
      once each minute.
     </p>
<p>      Note that <tt class="command">SYNC</tt> events are also generated on
      subscriber nodes.  Since they are not actually generating any
      data to replicate to other nodes, these <tt class="command">SYNC</tt>
      events are of not terribly much value.
     </p>
</dd>
<dt><span class="term"><tt class="option">-g</tt><i class="replaceable"><tt>group size</tt></i></span></dt>
<dd>
<p>      This controls the maximum <tt class="command">SYNC</tt> group size,
      <tt class="envar">sync_group_maxsize</tt>; defaults to 6.  Thus, if a
      particular node is behind by 200 <tt class="command">SYNC</tt>s, it
      will try to group them together into groups of a maximum size of
      <tt class="envar">sync_group_maxsize</tt>.  This can be expected to
      reduce transaction overhead due to having fewer transactions to
      <tt class="command">COMMIT</tt>.
     </p>
<p>      The default of 6 is probably suitable for small systems that can
      devote only very limited bits of memory to
      <span class="application">slon</span>.  If you have plenty of memory,
      it would be reasonable to increase this, as it will increase the
      amount of work done in each transaction, and will allow a
      subscriber that is behind by a lot to catch up more quickly.
     </p>
<p>      Slon processes usually stay pretty small; even with large value
      for this option, <span class="application">slon</span> would be
      expected to only grow to a few MB in size.
     </p>
<p>      The big advantage in increasing this parameter comes from
      cutting down on the number of transaction
      <tt class="command">COMMIT</tt>s; moving from 1 to 2 will provide
      considerable benefit, but the benefits will progressively fall
      off once the transactions being processed get to be reasonably
      large.  There isn't likely to be a material difference in
      performance between 80 and 90; at that point, whether
      &#8220;<span class="quote">bigger is better</span>&#8221; will depend on whether the
      bigger set of <tt class="command">SYNC</tt>s makes the
      <tt class="envar">LOG</tt> cursor behave badly due to consuming more
      memory and requiring more time to sortt.
     </p>
<p>      In <span class="productname">Slony-I</span> version 1.0, <span class="application">slon</span> will
      always attempt to group <tt class="command">SYNC</tt>s together to
      this maximum, which <span class="emphasis"><em>won't</em></span> be ideal if
      replication has been somewhat destabilized by there being very
      large updates (<span class="emphasis"><em>e.g.</em></span> - a single transaction
      that updates hundreds of thousands of rows) or by
      <tt class="command">SYNC</tt>s being disrupted on an origin node with
      the result that there are a few <tt class="command">SYNC</tt>s that
      are very large.  You might run into the problem that grouping
      together some very large <tt class="command">SYNC</tt>s knocks over a
      <span class="application">slon</span> process.  When it picks up
      again, it will try to process the same large grouped set of
      <tt class="command">SYNC</tt>s, and run into the same problem over and
      over until an administrator interrupts this and changes the
      <tt class="option">-g</tt> value to break this &#8220;<span class="quote">deadlock.</span>&#8221;
     </p>
<p>      In <span class="productname">Slony-I</span> version 1.0, the <span class="application">slon</span>
      instead adaptively &#8220;<span class="quote">ramps up</span>&#8221; from doing 1
      <tt class="command">SYNC</tt> at a time towards the maximum group
      size.  As a result, if there are a couple of
      <tt class="command">SYNC</tt>s that cause problems, the
      <span class="application">slon</span> will (with any relevant watchdog
      assistance) always be able to get to the point where it
      processes the troublesome <tt class="command">SYNC</tt>s one by one,
      hopefully making operator assistance unnecessary.
     </p>
</dd>
<dt><span class="term"><tt class="option">-o</tt><i class="replaceable"><tt>desired sync time</tt></i></span></dt>
<dd>
<p> A &#8220;<span class="quote">maximum</span>&#8221; time planned for grouped <tt class="command">SYNC</tt>s.</p>
<p> If replication is running behind, slon will gradually
     increase the numbers of <tt class="command">SYNC</tt>s grouped
     together, targetting that (based on the time taken for the
     <span class="emphasis"><em>last</em></span> group of <tt class="command">SYNC</tt>s) they
     shouldn't take more than the specified
     <tt class="envar">desired_sync_time</tt> value.</p>
<p> The default value for <tt class="envar">desired_sync_time</tt> is
     60000ms, equal to one minute. </p>
<p> That way, you can expect (or at least hope!) that you'll
      get a <tt class="command">COMMIT</tt> roughly once per minute. </p>
<p> It isn't <span class="emphasis"><em>totally</em></span> predictable, as it
     is entirely possible for someone to request a <span class="emphasis"><em>very
     large update,</em></span> all as one transaction, that can
     &#8220;<span class="quote">blow up</span>&#8221; the length of the resulting
     <tt class="command">SYNC</tt> to be nearly arbitrarily long.  In such a
     case, the heuristic will back off for the
     <span class="emphasis"><em>next</em></span> group.</p>
<p> The overall effect is to improve
      <span class="productname">Slony-I</span>'s ability to cope with
      variations in traffic.  By starting with 1 <tt class="command">SYNC</tt>, and gradually
      moving to more, even if there turn out to be variations large
      enough to cause <span class="productname">PostgreSQL</span> backends to
      crash, <span class="productname">Slony-I</span> will back off down to
      start with one sync at a time, if need be, so that if it is at
      all possible for replication to progress, it will.</p>
</dd>
<dt><span class="term"><tt class="option">-c</tt><i class="replaceable"><tt>cleanup cycles</tt></i></span></dt>
<dd>
<p>      The value <tt class="envar">vac_frequency</tt> indicates how often to
      <tt class="command">VACUUM</tt> in cleanup cycles.
     </p>
<p>      Set this to zero to disable
      <span class="application">slon</span>-initiated vacuuming. If you are
      using something like <span class="application">pg_autovacuum</span> to
      initiate vacuums, you may not need for slon to initiate vacuums
      itself.  If you are not, there are some tables
      <span class="productname">Slony-I</span> uses that collect a
      <span class="emphasis"><em>lot</em></span> of dead tuples that should be vacuumed
      frequently, notably <tt class="envar">pg_listener</tt>.
     </p>
<p> In <span class="productname">Slony-I</span> version 1.1, this changes a little; the
     cleanup thread tracks, from iteration to iteration, the earliest
     transaction ID still active in the system.  If this doesn't
     change, from one iteration to the next, then an old transaction
     is still active, and therefore a <tt class="command">VACUUM</tt> will
     do no good.  The cleanup thread instead merely does an
     <tt class="command">ANALYZE</tt> on these tables to update the
     statistics in <tt class="envar">pg_statistics</tt>.
     </p>
</dd>
<dt><span class="term"><tt class="option">-p</tt><i class="replaceable"><tt>PID filename</tt></i></span></dt>
<dd>
<p>      <tt class="envar">pid_file</tt> contains the filename in which the PID
      (process ID) of the <span class="application">slon</span> is stored.
     </p>
<p>      This may make it easier to construct scripts to monitor multiple
      <span class="application">slon</span> processes running on a single
      host.
     </p>
</dd>
<dt><span class="term"><tt class="option">-f</tt><i class="replaceable"><tt>config file</tt></i></span></dt>
<dd>
<p>      File from which to read <span class="application">slon</span> configuration.
     </p>
<p> This configuration is discussed further in <a href="runtime-config.html" title="Run-time Configuration">Run-time Configuration</a>.  If there are to be a complex set of
     configuration parameters, or if there are parameters you do not
     wish to be visible in the process environment variables (such as
     passwords), it may be convenient to draw many or all parameters
     from a configuration file.  You might either put common
     parameters for all slon processes in a commonly-used
     configuration file, allowing the command line to specify little
     other than the connection info.  Alternatively, you might create
     a configuration file for each node.</p>
</dd>
<dt><span class="term"><tt class="option">-a</tt><i class="replaceable"><tt>archive directory</tt></i></span></dt>
<dd><p>      <tt class="envar">archive_dir</tt> indicates a directory in which to
      place a sequence of <tt class="command">SYNC</tt> archive files for
      use in <a href="logshipping.html" title="13. Log Shipping - Slony-I with Files"> log shipping</a> mode.
     </p></dd>
<dt><span class="term"><tt class="option">-q</tt><i class="replaceable"><tt> quit based on SYNC provider </tt></i></span></dt>
<dd>
<p>      <tt class="envar">quit_sync_provider</tt> indicates which provider's
      worker thread should be watched in order to terminate after a
      certain event.  This must be used in conjunction with the
      <tt class="option">-r</tt> option below...
     </p>
<p> This allows you to have a <span class="application">slon</span>
     stop replicating after a certain point. </p>
</dd>
<dt><span class="term"><tt class="option">-r</tt><i class="replaceable"><tt> quit at event number </tt></i></span></dt>
<dd><p>      <tt class="envar">quit_sync_finalsync</tt> indicates the event number
      after which the remote worker thread for the provider above
      should terminate.  This must be used in conjunction with the
      <tt class="option">-q</tt> option above...
     </p></dd>
<dt><span class="term"><tt class="option">-l</tt><i class="replaceable"><tt> lag interval </tt></i></span></dt>
<dd><p>      <tt class="envar">lag_interval</tt> indicates an interval value such as
      <tt class="command"> 3 minutes </tt> or <tt class="command"> 4 hours </tt>
      or <tt class="command"> 2 days </tt> that indicates that this node is
      to lag its providers by the specified interval of time.  This
      causes events to be ignored until they reach the age
      corresponding to the interval.
     </p></dd>
</dl></div>
</div>
<div class="refsect1" lang="en">
<a name="id2551533"></a><h2>Exit Status</h2>
<p>   <span class="application">slon</span> returns 0 to the shell if it
   finished normally.  It returns -1 if it encounters any fatal error.
  </p>
</div>
</div></body>
</html>
